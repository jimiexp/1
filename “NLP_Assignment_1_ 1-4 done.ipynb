{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W57o0T4fhx0-"
   },
   "source": [
    "# NLP Assignment 1 (40% of grade): Sentiment Analysis from Tweets\n",
    "\n",
    "This coursework will involve you implementing functions for a text classifier, which you will train to identify the **sentiment expressed in a text** in a dataset of approx. 27,000 entries, which will be split into a 80%/20% training/test split.\n",
    "\n",
    "In this template you are given the basis for that implementation, though some of the functions are missing, which you have to fill in.\n",
    "\n",
    "Follow the instructions file **NLP_Assignment_1_Instructions.pdf** for details of each question - the outline of what needs to be achieved for each question is as below.\n",
    "\n",
    "You must submit all **ipython notebooks and extra resources you need to run the code if you've added them** in the code submission, and a **2 page report (pdf)** in the report submission on QMPlus where you report your methods and findings according to the instructions file for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8B9PiM4hx1A"
   },
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avbdRUnShx1B"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
    "    and performs the preprocessing.\"\"\"\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int((percentage * num_samples))\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)),label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n0pwAXjhx1C"
   },
   "source": [
    "# Question 1: Input and Basic preprocessing (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtC50niAhx1D"
   },
   "outputs": [],
   "source": [
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just positive or negative and the statement\n",
    "    # e.g. (label, statement)\n",
    "    label = data_line[1]  # Assuming the label is in the second column\n",
    "    statement = data_line[2]\n",
    "    return (label, statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySZwXPOVLe7L"
   },
   "source": [
    "# Simple preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKhTgJJ6y4ol"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    # Should return a list of tokens\n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    print(\"original:\", text)\n",
    "    # sentence segmentation - assume already done\n",
    "\n",
    "\n",
    "    # word tokenisation\n",
    "    text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", text) # separates punctuation at ends of strings\n",
    "    text = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", text) # separates punctuation at beginning of strings\n",
    "\n",
    "    # Convert multiple spaces to a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "    print(\"tokenising:\", text)\n",
    "    tokens = re.split(r\"\\s+\",text)\n",
    "    # normalisation - only by lower casing for now\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5ZVP3jJLoi7"
   },
   "source": [
    "# Simple feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FanEBi_HI9tD"
   },
   "outputs": [],
   "source": [
    "global_feature_dict = {}  # A global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens, binary=True):\n",
    "    \n",
    "    feature_vector = {}\n",
    "\n",
    "    for token in tokens:\n",
    "        # Update the feature vector\n",
    "        if binary:\n",
    "            feature_vector[token] = 1  # Binary representation (1 if present)\n",
    "        else:\n",
    "            # Bag-of-words count representation\n",
    "            feature_vector[token] = feature_vector.get(token, 0) + 1\n",
    "\n",
    "        # Update global feature dictionary\n",
    "        if token not in global_feature_dict:\n",
    "            global_feature_dict[token] = len(global_feature_dict)  # unique index for each token\n",
    "\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AASsJ80xhx1F"
   },
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "\n",
    "    sklearn_clf = SklearnClassifier(pipeline)  # Initialize SklearnClassifier\n",
    "    sklearn_clf.train(data)  # Train in place\n",
    "    if isinstance(sklearn_clf, SklearnClassifier):\n",
    "        print(\"Classifier successfully trained.\")\n",
    "\n",
    "    return sklearn_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwJzd217hx1G"
   },
   "source": [
    "# Question 3: Cross-validation (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4aKOj9PZACj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    results = []\n",
    "    fold_size = int(len(dataset) / folds) + 1\n",
    "\n",
    "    for i in range(0, len(dataset), int(fold_size)):\n",
    "        # Define start and end for the test fold\n",
    "        start = i\n",
    "        end = min(i + fold_size, len(dataset))  # Ensure end does not exceed dataset length\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        test_data = dataset[start:end]\n",
    "        train_data = dataset[:start] + dataset[end:]\n",
    "\n",
    "        # Train classifier on training data\n",
    "        classifier = train_classifier(train_data)\n",
    "\n",
    "        # Get true labels and predictions for test data\n",
    "        true_labels = [label for _, label in test_data]\n",
    "        predictions = predict_labels([text for text, _ in test_data], classifier)\n",
    "\n",
    "        # Calculate metrics for this fold\n",
    "        precision = precision_score(true_labels, predictions, pos_label='positive')\n",
    "        recall = recall_score(true_labels, predictions, pos_label='positive')\n",
    "        f1 = f1_score(true_labels, predictions, pos_label='positive')\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        precision = precision_score(true_labels, predictions, pos_label='positive')\n",
    "        recall = recall_score(true_labels, predictions, pos_label='positive')\n",
    "        f1 = f1_score(true_labels, predictions, pos_label='positive')\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "\n",
    "\n",
    "        # Append results for this fold\n",
    "        fold_result = {\n",
    "            'Fold': i // fold_size + 1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        results.append(fold_result)\n",
    "\n",
    "        print(\"Fold %d - Precision: %.4f, Recall: %.4f, F1 Score: %.4f, Accuracy: %.4f\" %\n",
    "              (i // fold_size + 1, precision, recall, f1, accuracy))\n",
    "\n",
    "        print(f\"Fold {i // fold_size + 1} Classification Report:\")\n",
    "        print(classification_report(true_labels, predictions, target_names=['negative', 'positive']))\n",
    "\n",
    "    # Calculate average results over all folds\n",
    "    avg_results = {metric: np.mean([fold[metric] for fold in results]) for metric in results[0]}\n",
    "    avg_results['Fold'] = 'Average'\n",
    "    results.append(avg_results)  # Append the average row to the results\n",
    "\n",
    "    print(\"\\nAverage Cross-Validation Results\")\n",
    "    print(\"Precision: %.4f, Recall: %.4f, F1 Score: %.4f, Accuracy: %.4f\" %\n",
    "          (avg_results['precision'], avg_results['recall'], avg_results['f1_score'], avg_results['accuracy']))\n",
    "    cv_results=results\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPRN3J66hx1H"
   },
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdOXLDXBhx1H",
    "outputId": "b4a9ea1c-7f25-4e2d-e2d8-60ff1a574ee3"
   },
   "outputs": [],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path)\n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_p0g8Pahx1H",
    "outputId": "85661bdc-56a8-4f75-c84b-170ffa758ef1"
   },
   "outputs": [],
   "source": [
    "results = cross_validate(train_data, 10)  # will work and output overall performance of p, r, f-score when cv implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "XlPgjdqrg64i",
    "outputId": "41df5fc1-8730-4588-d21e-7d2f5ae6e19a"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCqK_7tYhx1H"
   },
   "source": [
    "# Question 4: Error Analysis (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lv6-qOLDhx1I"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# a function to make the confusion matrix readable and pretty\n",
    "def confusion_matrix_heatmap(y_test, preds, labels):\n",
    "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
    "    # pass labels to the confusion matrix function to ensure right order\n",
    "    # cm = metrics.confusion_matrix(y_test, preds, labels)\n",
    "    cm = metrics.confusion_matrix(y_test, preds, labels=labels)\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels( labels, rotation=45)\n",
    "    ax.set_yticklabels( labels)\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            text = ax.text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # fix for mpl bug that cuts off top/bottom of seaborn viz:\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show() # ta-da!\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TupxciweroPD"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def error_output (dataset, folds):\n",
    "    results = []\n",
    "    fold_size = int(len(dataset) / folds) + 1\n",
    "    false_predictions = []\n",
    "    misclassified_samples = []\n",
    "\n",
    "\n",
    "    for i in range(0, len(dataset), int(fold_size)):\n",
    "        start = i\n",
    "        end = min(i + fold_size, len(dataset))  # Ensure end does not exceed dataset length\n",
    "\n",
    "        # Split the data into training and test sets\n",
    "        test_data = dataset[start:end]\n",
    "        train_data = dataset[:start] + dataset[end:]\n",
    "\n",
    "        # Train classifier on training data\n",
    "        classifier = train_classifier(train_data)\n",
    "\n",
    "        # Get true labels and predictions for test data\n",
    "        true_labels = [label for _, label in test_data]\n",
    "        texts = [text for text, _ in test_data]\n",
    "        predictions = predict_labels(texts, classifier)\n",
    "\n",
    "        # Collect false predictions for the first fold\n",
    "        if i == 0:  # First fold\n",
    "            false_predictions = [\n",
    "                {'Text': text, 'True Label': true_label, 'Predicted Label': pred}\n",
    "                for text, true_label, pred in zip(texts, true_labels, predictions)\n",
    "                if true_label != pred\n",
    "            ]\n",
    "            for text, true_label, pred_label in zip(texts, true_labels, predictions):\n",
    "              if true_label != pred_label:\n",
    "                misclassified_samples.append({\n",
    "                    'Fold': i // fold_size + 1,\n",
    "                    'Text': text,\n",
    "                    'True Label': true_label,\n",
    "                    'Predicted Label': pred_label\n",
    "                })\n",
    "\n",
    "        # Write false predictions from the first fold to a file\n",
    "    with open(\"false_predictions_fold_1.txt\", \"w\") as f:\n",
    "        for fp in false_predictions:\n",
    "            f.write(f\"Text: {fp['Text']}\\nTrue Label: {fp['True Label']}\\nPredicted Label: {fp['Predicted Label']}\\n\\n\")\n",
    "    confusion_matrix_heatmap(true_labels, predictions, labels=['positive', 'negative'])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XrKN-_oAHmQt",
    "outputId": "4d3e2bb9-c3b6-4b8a-bac0-a477fb2301e3"
   },
   "outputs": [],
   "source": [
    "error_output (train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sc21jHQshcz",
    "outputId": "f36844e0-d8fa-4c38-8e19-c9e4700ff063"
   },
   "outputs": [],
   "source": [
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "# Loop to collect false positives and false negatives for the positive label\n",
    "for text, true_label, pred_label in zip(test_texts, test_labels, predicted_labels):\n",
    "    if true_label == 'negative' and pred_label == 'positive':  # False Positive\n",
    "        false_positives.append((text, true_label, pred_label))\n",
    "    elif true_label == 'positive' and pred_label == 'negative':  # False Negative\n",
    "        false_negatives.append((text, true_label, pred_label))\n",
    "\n",
    "# Print or save to file\n",
    "print(\"False Positives:\")\n",
    "for fp in false_positives:\n",
    "    print(f\"Text: {fp[0]} | True Label: {fp[1]} | Predicted Label: {fp[2]}\")\n",
    "\n",
    "print(\"\\nFalse Negatives:\")\n",
    "for fn in false_negatives:\n",
    "    print(f\"Text: {fn[0]} | True Label: {fn[1]} | Predicted Label: {fn[2]}\")\n",
    "\n",
    "# Save to file for further analysis\n",
    "with open(\"error_analysis.txt\", \"w\") as f:\n",
    "    for fp in false_positives:\n",
    "        f.write(f\"Text: {fp[0]} | True Label: {fp[1]} | Predicted Label: {fp[2]}\\n\")\n",
    "    for fn in false_negatives:\n",
    "        f.write(f\"Text: {fn[0]} | True Label: {fn[1]} | Predicted Label: {fn[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqn4H3fMhx1I"
   },
   "source": [
    "# Questions 5: Optimising pre-processing and feature extraction (30 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OujeCY54hx1I"
   },
   "source": [
    "**Note:** it is advisable to implement question 5 in a separate notebook where you further develop the pre-processing and feature extraction functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug8zGEKmhx1I"
   },
   "outputs": [],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = False  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95zYYxq0hx1I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DgFz4C1hx1I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
